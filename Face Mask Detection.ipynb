{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyOQZ9JN8e0R45dOMKFZJIxV"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"zAspvtrPp-Sp"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import cv2\n","import json\n","import matplotlib.pyplot as plt\n","import os\n","import random\n","import seaborn as sns\n","from keras.models import Sequential\n","from keras import optimizers\n","from keras import backend as B\n","from keras.layers import Dense, Dropout, Activation, Flatten\n","from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n","from sklearn.model_selection import train_test_split\n","from keras.preprocessing.image import ImageDataGenerator"]},{"cell_type":"code","source":["directory = \"../input/face-mask-detection-dataset/Medical mask/Medical mask/Medical Mask/annotations\"\n","imagedirectory = \"../input/face-mask-detection-dataset/Medical mask/Medical mask/Medical Mask/images\"\n","df = pd.read_csv(\"../input/face-mask-detection-dataset/train.csv\")\n","df_test = pd.read_csv(\"../input/face-mask-detection-dataset/submission.csv\")"],"metadata":{"id":"pKpWliOGrFGg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# The creation of helper functions would be necessary task to refine and prepare the data:\n","cvv = cv2.dnn.readNetFromCaffe('weights.caffemodel')         #This function is used to read a deep learning network represented in the Caffe framework's format..the pathway should be actual to file in the .caffemodel"],"metadata":{"id":"eLJz08eZrUAI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def JSONpath(filePathandName):\n","    with open(filePathandName,'r') as f:\n","        return json.load(f)    #The JSONpath function retrieves the json file containing the bounding box data in the training dataset.\n","def adjust_gamma(image, gamma=1.0):\n","    invertGamma = 1.0 / gamma\n","    table = np.array([((i / 255.0) ** invertGamma) * 255 for i in np.arange(0, 256)])\n","    return cv2.LUT(image.astype(np.uint8), table.astype(np.uint8))        #The adjust_gamma function is a non-linear operation used to encode and decode luminance or tristimulus values in video or still image systems.If gamma <1, the image will shift to the darker end of the spectrum and when gamma> 1, there will be more light in the image."],"metadata":{"id":"TYs--fKAswMq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["jsonfile = []\n","for i in os.listdir(directory):\n","  jsonfile.append(JSONpath(os.path.join(directory,i)))\n","jsonfile[0]   #creating a list of JSON file paths, and then accessing the first JSON file path in the list."],"metadata":{"id":"LHDykTmntdw6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = pd.read_csv(\"train.csv\")\n","df.head()"],"metadata":{"id":"buKgC_VluUU5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Using the mask and the non_mask labels,\n","#the bounding box data of the json files is extracted. The faces of a particular image are extracted and stored in the data list with its tag for the learning process.\n","#iterating over unique values in a column named \"name\" in a DataFrame (df). For each unique value,constructing a JSON file name by appending \".json\" to it. Then,retrieving annotations from the JSON file using a function called JSONpath() and filtering based on the \"classname\" attribute. If the \"classname\" is in the \"mask\" list,extracting a bounding box and corresponding image from another directory using OpenCV (cv2).resizing the image to a specified size (img_size) and then appending the resized image and its label to a data list.\n","data = []\n","imgsize = 124\n","mask = ['face_with_mask']\n","non_mask = ['face_no_mask']\n","labels = {'mask' : 0, 'without mask' : 1}\n","for i in df[\"name\"].unique():\n","  f = i + \".json\"\n","  for j in JSONpath(os.path.join(directory,f)).get(\"Annotations\"):\n","    if j[\"classname\"] in mask:\n","      a,b,c,d = j[\"BoundingBox\"]\n","      img = cv2.imread(os.path.join(imagedirectory,i),1)\n","      img = img[b:d,a:c]\n","      img = cv2.resize(img,(imgsize,imgsize))\n","      data.append([img,labels[\"mask\"]])\n","\n","      #the classname is not in the \"non_mask\" list. When this condition is met,extract the bounding box coordinates from the annotation and use OpenCV to read, crop, and resize the corresponding image. Then,append the resized image along with its label to the \"data\" list. Finally, you shuffle the data.\n","    if j[\"classname\"] in non_mask:\n","      a,b,c,d = j[\"BoundingBox\"]\n","      img = cv2.imread(os.path.join(imagedirectory,i),1)\n","      img = img[b:d,a:c]\n","      img = cv2.resize(img,(imgsize,imgsize))\n","      data.append([img,labels[\"without mask\"]])\n","random.shuffle(data)"],"metadata":{"id":"o2bd44f8uaUL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot = []     #assign an empty list named as plot\n","for face in data:     #iterate over the dat for the faces with mask...if the face selected has value as 0 then masked else non masked..construct the plot of the data stored in the list plot\n","    if(face[1] == 0):\n","        plot.append(\"Mask\")\n","    else:\n","        plot.append(\"No Mask\")\n","sns.countplot(plot)"],"metadata":{"id":"cav3zYRawa_A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#In any different model training we have to balanced out the different features used but here since we are usingssd model(vgg16),it doesn't matter much\n","X = []\n","Y = []\n","for features,label in data:\n","    X.append(features)\n","    Y.append(label)\n","\n","X = np.array(X)/255.0\n","X = X.reshape(-1,124,124,3)\n","Y = np.array(Y)"],"metadata":{"id":"lshaqNfCwbC0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Model Building\n","model = Sequential()\n","model.add(Conv2D(32,(3,3),padding = \"same\",activation = \"relu\",input_shape = (124,124,3)))\n","model.add(Conv2D(64,(3,3),activation = \"relu\"))\n","model.add(Conv2D(128, (3, 3), activation='relu'))\n","model.add(Conv2D(256, (3, 3), activation='relu'))\n","model.add(MaxPooling2D(pool_size=(2,2)))\n","model.add(Dropout(0.25))\n","model.add(Flatten())\n","model.add(Dropout(0.5))\n","model.add(Dense(50, activation='relu'))\n","model.add(Dropout(0.5))\n","model.add(Dense(1, activation='sigmoid'))\n","model.compile(optimizer = \"rmsprop\"loss='binary_crossentropy', optimizer='adam' ,metrics=['accuracy'])"],"metadata":{"id":"3ZIeYleGygt2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Model Training\n","x_train,x_test,y_train,y_test = train_test_split(X,Y,train_size = 0.9,random = 123)\n","datagenerator = ImageDataGenerator(featurewise_center=False,samplewise_std_normalization=False,zca_whitening=False,rotation_range=35,width_shift_range=0.1,height_shift_range=0.1,horizontal_flip=True,vertical_flip=False)\n","datagenerator.fit(x_train)\n","modelfit = model.fit_generator(datagenerator.flow(x_train,y_train,batch_size = 32),steps_per_epoch = x_train.shape[0]//32,epochs = 50,verbose = 1,validation_data = (x_test,y_test))\n"],"metadata":{"id":"1y8juJ0-zzt3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["testing = ['1114.png','1554.jpg', '1072.jpg','0012.jpg','0357.jpg','1374.jpg','1234.jpg','1323.jpg']\n","gamma = 2\n","figure = plt.figure((figsize = (14,14)))\n","rows = 4\n","cols = 2\n","axes = []\n","assigned = {'0' : 'MASK', '1' : 'NON MASK'}\n","for j,m in enumerate(testing):\n","  image =  cv2.imread(os.path.join(imagedirectory,m),1)\n","  image =  adjust_gamma(image, gamma=gamma)\n","  (c, d) = image.shape[:2]\n","  blob = cv2.dnn.blobFromImage(cv2.resize(image, (300,300)), 1.0, (300, 300), (104.0, 177.0, 123.0))\n","  cvv.setInput(blob)\n","  detections = cvv.forward()\n","\n","#For each image filename in the testing list:read the image using OpenCV (cv2.imread()),adjust the gamma of the image using a function named adjust_gamma().resize the image to a size of 300x300 pixels and convert it into a blob suitable for input to a neural network using OpenCV's cv2.dnn.blobFromImage() function.set the input of the neural network (cvNet.setInput(blob)).perform forward pass inference on the neural network (cvNet.forward()), presumably for object detection."],"metadata":{"id":"9VEROqDc2lEZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for i in range(0,detections.shape[2]):\n","  try:\n","    boxi = detections[0,0,i,3:7] * np.array([d,c,d,c])\n","    (startX, startY, endX, endY) = boxi.astype(\"int\")\n","    frame = image[startY:endY, startX:endX]\n","    confidence = detections[0, 0, i, 2]\n","    if confidence > 0.2:\n","      m = cv2.resize(frame(imgsize,imgsize))\n","      m = np.array(m) / 255.0\n","      m = m.reshape(1,124,124,3)\n","      result = model.predict(m)\n","      if result > 0.5:\n","        label_Y = 1\n","      else:\n","        label_Y = 0\n","cv2.rectangle(image, (startX, startY), (endX, endY), (0, 0, 255), 2)\n","cv2.putText(image,assigned[str(label_Y)] , (startX, startY-10), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (36,255,12), 2)\n","  except:pass\n","axes.append(figure.add_subplot(rows,cols,j + 1))\n","plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n","plt.axis('on')\n","plt.gca().add_patch(plt.Rectangle((boxi[0], boxi[1]), boxi[2] - boxi[0], boxi[3] - boxi[1], fill=False, edgecolor='green', linewidth=6))\n","plt.show()\n","#Bounding box coordinates and confidence score are extracted.\n","#If the confidence score exceeds a threshold (0.2), further processing occurs:\n","#The detected object is resized to a specified size.\n","#Normalization and reshaping are applied for prediction.\n","#Based on the prediction result, a bounding box and label are drawn on the original image.\n","#The original image with overlaid bounding boxes is visualized using Matplotlib."],"metadata":{"id":"2JqXa6jE7uia"},"execution_count":null,"outputs":[]}]}